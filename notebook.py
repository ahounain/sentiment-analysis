# -*- coding: utf-8 -*-
"""CS670/470 Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PKTb-8GAVurfz8enZUTT6zimQ9aJ0Bo5
"""

# Commented out IPython magic to ensure Python compatibility.
# #First step to install the dataset library
# %%capture
# !pip install datasets

import os

# Remove the cache directory
os.system('rm -rf ~/.cache/huggingface/datasets')

#for the access to the hugging face dataset we use this token
from huggingface_hub import login

login("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")

from datasets import get_dataset_config_names

#check the languages in the dataset
configs = get_dataset_config_names("mozilla-foundation/common_voice_13_0")
print(configs)

from datasets import load_dataset

# Load the English split of the dataset
cv_13 = load_dataset("mozilla-foundation/common_voice_13_0", "en", split="train[:100]")
#PROBLEM -> ITS TO HEAVY TO BE INSTALLED

from datasets import load_dataset

# Languages and configurations
languages = ["en", "hi", "fa"]  # English, Hindi, Farsi
num_samples = 1000  # Number of examples per dataset
required_columns = ["accent", "age", "gender", "sentence", "path", "audio"]

# Specify the directory where you want to save the datasets
save_directory = "/content/drive/MyDrive/A1_AI"

# Dictionary to store datasets
datasets = {}

# Load, subset, and filter datasets for each language using streaming
for lang in languages:
    print(f"Processing {lang}...")

    # Load the dataset in streaming mode
    ds = load_dataset("mozilla-foundation/common_voice_13_0", lang, split="train", streaming=True)

    # Process the first `num_samples` examples
    ds_iterable = iter(ds)
    subset = [next(ds_iterable) for _ in range(num_samples)]

    # Convert to a list of dictionaries with only the required columns
    subset = [{key: sample[key] for key in required_columns} for sample in subset]

    # Store the dataset in memory (or convert to a Dataset object if needed)
    datasets[lang] = subset

    # Print a sample for verification
    print(f"Sample from {lang}: {subset[0]}")

# Save the datasets locally
import json
import os
import numpy as np

for lang, subset in datasets.items():
    # Create the directory if it doesn't exist
    os.makedirs(save_directory, exist_ok=True)

    # Save each dataset as a JSON file
    save_path = f"{save_directory}/{lang}_10k_dataset.json"
    with open(save_path, "w") as f:
        json.dump(subset, f)
    print(f"Saved {lang} dataset to {save_path}.")

from google.colab import drive
drive.mount('/content/drive')

'''
from datasets import load_dataset

# Load the dataset with shuffling and limiting to N examples
cv_13 = load_dataset("mozilla-foundation/common_voice_13_0", "en", split="train").shuffle(seed=42).select(range(1000))

print(f"Number of samples loaded: {len(cv_13)}")
'''

#just an example to see all the labels for an entry of the dataset
print(cv_13[0])

#Here we want to see an audio example
from datasets import Audio
from IPython.display import Audio as IpyAudio

# Cast the 'audio' column to audio type
cv_13 = cv_13.cast_column("audio", Audio())

# Extract the first audio example
audio_data = cv_13[0]["audio"]

# Play the audio
display(IpyAudio(audio_data["array"], rate=audio_data["sampling_rate"]))

"""#Whisper Model"""

!pip install openai-whisper

from datasets import get_dataset_config_names, load_dataset, Audio
from IPython.display import Audio as IpyAudio
import whisper

# Cast the 'audio' column to Audio type
cv_13 = cv_13.cast_column("audio", Audio())

# Extract the first audio example
audio_data = cv_13[0]["audio"]
text_ground_truth = cv_13[0]["sentence"]

# Play the audio to verify
display(IpyAudio(audio_data["array"], rate=audio_data["sampling_rate"]))

# Load the Whisper model
model = whisper.load_model("base")  # Use "tiny", "small", etc., based on resources

# Save the audio data to a temporary file
import tempfile
import torchaudio
import torch

temp_audio_file = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
torchaudio.save(temp_audio_file.name, torch.tensor(audio_data["array"]).unsqueeze(0), audio_data["sampling_rate"])

# Transcribe the audio using Whisper
result = model.transcribe(temp_audio_file.name)
predicted_text = result["text"]

# Get language
language = model.transcribe(temp_audio_file.name, task = "transcribe")
language = language.get("language", None)
print(language)


# Display the prediction and ground truth
print("Ground Truth:", text_ground_truth)
print("Prediction:", predicted_text)

# Clean up the temporary file
temp_audio_file.close()

#HOW TO CLEAN UP THE DATASET TO USE THE ELEMENTS
import pandas as pd

# Convert the dataset to a pandas DataFrame
df = pd.DataFrame(cv_13)

'''
# Keep only the desired columns -> we dont need to eliminate columns yet
desired_columns = ['client_id', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender']
df = df[desired_columns]
'''

# Remove rows where `age` or `gender` is empty
df_filtered = df[(df['age'] != "") & (df['gender'] != "")]

# View the filtered DataFrame
print(df_filtered.head())

# Optional: Save to a CSV file
#df_filtered.to_csv("filtered_dataset.csv", index=False)

#
# TRAIN MODEL ON MOZILLA COMMON VOICE
#
#
!pip install transformers
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, TrainingArguments, Trainer

# define model
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
#
# REPLACE LANG WITH LANGUAGE YOU WANT TO TRAIN ON
#
common_voice = load_dataset("mozilla-foundation/common_voice_13_0", "hi", split="train")
#

eval_dataset = load_dataset("mozilla-foundation/common_voice_13_0", "hi", split="test")

# reduce dataset size (session crashes bc not enough ram :( ))
common_voice = common_voice.select(range(int(len(common_voice) * 0.1)))
eval_dataset = eval_dataset.select(range(int(len(eval_dataset) * 0.1)))

# preprocess dataset
def preprocess_func(examples):

  # cast audio column to audio type
  audio_arrays = [x["array"] for x in examples["audio"]]

  # define max length for audio and text
  max_length_audio = 16000 * 5 # 5 second audio @ 16kHz sampling rate
  max_length_text = 128

  # process audio using wav2vecprocessor
  processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
  inputs = processor(
      audio_arrays,
      sampling_rate = 16000,
      return_tensors = "pt",
      padding = True,
      truncation = True,
      max_length = max_length_audio,
      max_length_labels = max_length_text,
      )
  '''
  with processor:
    labels = processor(
        examples["sentence"],
        padding = True,
        truncation = True,
        return_tensors = "pt",
        max_length = max_length_text,
        ).input_ids
  inputs["labels"] = labels # add labels key to input dict
  '''
  return inputs


# preprocess the datasets

common_voice = common_voice.map(
    preprocess_func,
    remove_columns = common_voice.column_names,
    batched = True
)
eval_dataset = eval_dataset.map(
    preprocess_func,
    remove_columns = eval_dataset.column_names,
    batched = True
)






# define training args
training_args = TrainingArguments(
    output_dir="./wav2vec2-base-960h-finetuned",
    run_name = "fine-tuning-run",
    per_device_train_batch_size = 4,
    per_device_eval_batch_size = 4,
    gradient_accumulation_steps = 8,
    eval_strategy = "steps",
    num_train_epochs = 3,
    fp16 = True,
    remove_unused_columns = False,
)

# define the trainer
print(common_voice[0].keys)
print(eval_dataset[:5])
trainer = Trainer(
    model = model,
    args = training_args,
    train_dataset = cv_13,
    eval_dataset = eval_dataset,
)
trainer.train()

"""#Model Above
the model is taking too long check why its happening
epochs, dataloader, ...
"""

#
# TRANSLATE SPOKEN WORD INTO TEXT FROM A GIVEN AUDIO FILE.
#
#

import torch
import librosa
import soundfile as sf

# audio file example
audio_data = cv_13[0]["audio"]

# load pretrained model + processor
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

# save audio data as a WAV file so librosa can use it
sf.write("temp_audio.wav", audio_data["array"], audio_data["sampling_rate"])
audio_input, sample_rate = librosa.load("temp_audio.wav", sr = 16000)
input_values = processor(audio_input, sampling_rate = sample_rate, return_tensors = "pt").input_values

# perform inference
with torch.no_grad():
    logits = model(input_values).logits

# use argmax to get the predicted IDs
predicted_ids = torch.argmax(logits, dim = -1)

# decode the IDs to text
transcription = processor.batch_decode(predicted_ids)
print(transcription)

"""#Simple Multilingual BERT Model"""

from transformers import pipeline

# Load a sentiment-analysis pipeline with mBERT
sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Example texts in different languages
texts = [
    "Das ist ein fantastischer Tag!",  # German: "This is a fantastic day!"
    "¡Hoy es un día terrible!",       # Spanish: "Today is a terrible day!"
    "今日は良い日です！"               # Japanese: "Today is a good day!"
]

# Analyze sentiment
results = sentiment_pipeline(texts)

# Print results
for text, result in zip(texts, results):
    print(f"Text: {text}\nSentiment: {result}\n")