I trained a model on the RAVDESS dataset which is a dataset of actors performing various emotions, and I extracted MFCC features,  and took 40 coefficents per frame of audio. I resampled the audio to 16kHz and padded sequences to 200 frames, and added one hot encoding to emotion labels (happy, calm, sad, etc.), did a 20 percent test 80 percent train split for evaluation, (0.75862 percent test accuracy), and for the model I built a Sequential Neural Network by converting the 2D MFCC features to 1D, and had 2 dense hidden layers with ReLU activation using 128 neurons for the first, 64 neurons for the second, and then 6 neurons for the output layer. 